## AdaBoost（Adaptive boosting）

**优点**：泛化错误率低，一遍吗，可以 应用在大部分分类器上，无参数调整

**缺点**： 对离群点敏感

**适用数据类型**：数值型和标称型数据


在boosting中，不同的分类器通过串行训练而活得，每个训练器都根据以训练出的分类器的性能来进行训练。boosting是通过集中关注被已有分类器错分的那些数据来获得新的分类器。

boosting中分类器的权重**不相等**，每个权重带标的是其对应分类器在上一轮迭代中的成功度。



训练过程：

1. 训练数据中的每个样本，并赋予其一个权重，这些权重构成了向量$D$，一开始，这些权重初始化成相等值。
2. 在训练数据上训练出一个弱分类器并计算该分类器的错误率，然后在同一数据集上再次训练弱分类器。
3. 第二次训练后，重新调整每个样本的权重，降低第一次分对的样本的权重将会降低，分错的样本的权重将会提高。
4. AdaBoost为每个分类器分类了一个权重值alpha，基于弱分类器的错误率进行计算。
    错误率定义：
    $$
    \epsilon=\frac{未分类正确的样本数目}{所有样本数目}alpha计算公式如下：
    $$
    alpha计算公式如下：
    $$
    \alpha=\frac{1}{2}\ln\frac{1-\epsilon}{\epsilon}
    $$

5. 计算出alpha值后，对$D$进行更新。
    如果样本被正确分类：
    $$
    D_i^{(t+1)}=\frac{D_i^{(t)}e^{-\alpha}}{Sum(D)}
    $$
    如果样本被错分：
    $$
    D_i^{(t+1)}=\frac{D_i^{(t)}e^\alpha}{Sum(D)}
    $$

6. 迭代，直到训练错误率达到0或弱分类器数目达到用户指定值。
