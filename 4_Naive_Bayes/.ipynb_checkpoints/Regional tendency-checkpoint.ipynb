{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import feedparser\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词转化为向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建实验样本\n",
    "def loadDataSet():\n",
    "    # 词条集合\n",
    "    postingList= [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\\\n",
    "                  ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\\\n",
    "                  ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\\\n",
    "                  ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\\\n",
    "                  ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\\\n",
    "                  ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    # 标签集合\n",
    "    classVec = [0, 1, 0, 1, 0, 1]    # 1-侮辱性文字，0-正常言论\n",
    "    return postingList, classVec\n",
    "\n",
    "# 创建所有词的列表\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)    # 并集\n",
    "    return list(vocabSet)\n",
    "\n",
    "# 判断 vocabList 中的单词在 inputSet 中是否出现\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)    # 设置与词汇表等长的0向量\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print('the word: %s is not in my Vocabulary!' % word)\n",
    "    return returnVec\n",
    "\n",
    "# 词袋模型\n",
    "# 判断 vocabList 中的单词在 inputSet 中是否出现\n",
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)    # 设置与词汇表等长的0向量\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "        #else:\n",
    "        #    print('the word: %s is not in my Vocabulary!' % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文件解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textParse(bigString):\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]    # 去除长度太短的无效字符串，取消首字母大写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯分类器训练函数\n",
    "伪代码如下：  \n",
    "计算每个类别中文档数目  \n",
    "对每篇文章训练文档：  \n",
    "&ensp;&ensp;&ensp;&ensp;对每个类别：  \n",
    "&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;如果词条出现在文档中——>增加该词条计数值  \n",
    "&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;增加所有词条计数值  \n",
    "对每个类别：  \n",
    "&ensp;&ensp;&ensp;&ensp;对每个词条：  \n",
    "&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;将该词条的数目除以总词条的数目的到条件概率  \n",
    "返回每个类别的条件概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)    # 计算侮辱性文章的概率\n",
    "    # 初始化概率, 防止概率为0，设置词初试次数为1，分母为加上类别数量，这里加2\n",
    "    p0Num = np.ones(numWords)\n",
    "    p1Num = np.ones(numWords)\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "    for i in range(numTrainDocs):    # 遍历文档\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]    # 侮辱性文章包含的词汇加1\n",
    "            p1Denom += sum(trainMatrix[i])    # 总词数加1\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = np.log(p1Num/p1Denom)    # 除以类别总词数, 为了防止数字过小下溢，使用log函数\n",
    "    p0Vect = np.log(p0Num/p0Denom)\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯分类函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)    # 转换为log之后原先的乘法变加法,条件独立的乘法变sum\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 找到出现次数最多的30个词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcMostFreq(vocabList, fullText):\n",
    "    freqDict = {}\n",
    "    # 统计所有词出现的频次\n",
    "    for token in vocabList:\n",
    "        freqDict[token] = fullText.count(token)\n",
    "    # 根据出现次数从高到低排序词\n",
    "    sortedFreq = sorted(freqDict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedFreq[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSS分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localWords(feed1, feed0):\n",
    "    docList = []\n",
    "    classList = []\n",
    "    fullText = []\n",
    "    minLen = min(len(feed1['entries']), len(feed0['entries']))\n",
    "    # 访问RSS，存储数据\n",
    "    for i in range(minLen):\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)\n",
    "    # 移除出现最多的30个词\n",
    "    top30Words = calcMostFreq(vocabList, fullText)\n",
    "    for pairW in top30Words:\n",
    "        if pairW[0] in vocabList:\n",
    "            vocabList.remove(pairW[0])\n",
    "    trainingSet = list(range(2*minLen))\n",
    "    testSet = []\n",
    "    # 随机选择5个做测试集\n",
    "    for i in range(5):\n",
    "        randIndex = int(np.random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        # 构建词向量，加入到测试集\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    # 计算概率\n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))\n",
    "    errorCount = 0\n",
    "    # 遍历测试集\n",
    "    for docIndex in testSet:\n",
    "        # 构建词向量\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        # 分类并检测是否正确\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print('the error rate is: ', float(errorCount)/len(testSet))\n",
    "    return vocabList, p0V, p1V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 返回高于某个阈值的所有词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopWords(ny, sf):\n",
    "    vocabList, p0V, p1V = localWords(ny, sf)\n",
    "    topNY = []\n",
    "    topSF = []\n",
    "    for i in range(len(p0V)):\n",
    "        if p0V[i] > -6.0:\n",
    "            topSF.append((vocabList[i], p0V[i]))\n",
    "        if p1V[i] > -6.0:\n",
    "            topNY.append((vocabList[i], p0V[i]))\n",
    "    sortedSF = sorted(topSF, key=lambda pair:pair[1], reverse=True)\n",
    "    print('SF*SF*SF*SF*SF*SF*SF*SF*SF*SF*SF*SF*SF*SF*SF*SF*SF*SF*SF')\n",
    "    for item in sortedSF:\n",
    "        print(item[0])\n",
    "    sortedNY = sorted(topNY, key=lambda pair:pair[1], reverse=True)\n",
    "    print('NY*NY*NY*NY*NY*NY*NY*NY*NY*NY*NY*NY*NY*NY*NY*NY*NY*NY*NY')\n",
    "    for item in sortedNY:\n",
    "        print(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny = feedparser.parse('https://newyork.craigslist.org/search/res?format=rss')\n",
    "sf = feedparser.parse('https://sfbay.craigslist.org/search/apa?format=rss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.2\n",
      "SF*SF*SF*SF*SF*SF*SF*SF*SF*SF*SF*SF*SF*SF*SF*SF*SF*SF*SF\n",
      "features\n",
      "unit\n",
      "easy\n",
      "located\n",
      "two\n",
      "vineyard\n",
      "santa\n",
      "level\n",
      "location\n",
      "beautiful\n",
      "luxury\n",
      "family\n",
      "house\n",
      "floor\n",
      "building\n",
      "block\n",
      "city\n",
      "san\n",
      "deck\n",
      "that\n",
      "park\n",
      "remodeled\n",
      "modern\n",
      "ave\n",
      "bay\n",
      "clara\n",
      "access\n",
      "casa\n",
      "hill\n",
      "walking\n",
      "walk\n",
      "light\n",
      "offers\n",
      "com\n",
      "steps\n",
      "http\n",
      "cupertino\n",
      "viva\n",
      "make\n",
      "full\n",
      "downtown\n",
      "gorgeous\n",
      "contact\n",
      "call\n",
      "school\n",
      "schedule\n",
      "tour\n",
      "set\n",
      "living\n",
      "heights\n",
      "today\n",
      "bustling\n",
      "visit\n",
      "completely\n",
      "between\n",
      "perfectly\n",
      "dining\n",
      "sands\n",
      "sahara\n",
      "well\n",
      "market\n",
      "napa\n",
      "just\n",
      "country\n",
      "broadway\n",
      "when\n",
      "bed\n",
      "single\n",
      "backyard\n",
      "distance\n",
      "now\n",
      "ample\n",
      "counter\n",
      "bart\n",
      "bright\n",
      "starbird\n",
      "wood\n",
      "lot\n",
      "hardwood\n",
      "renovated\n",
      "built\n",
      "architecture\n",
      "wine\n",
      "floors\n",
      "hyde\n",
      "jose\n",
      "station\n",
      "close\n",
      "overlooking\n",
      "baxter\n",
      "updated\n",
      "fully\n",
      "has\n",
      "francisco\n",
      "big\n",
      "beach\n",
      "pacific\n",
      "space\n",
      "bedrooms\n",
      "2313\n",
      "jackson\n",
      "available\n",
      "furnished\n",
      "neighborhood\n",
      "open\n",
      "halfway\n",
      "bathroom\n",
      "quiet\n",
      "cpmc\n",
      "studio\n",
      "university\n",
      "2pm\n",
      "noe\n",
      "deserve\n",
      "feel\n",
      "webster\n",
      "stunning\n",
      "capitola\n",
      "berkeley\n",
      "entertaining\n",
      "stay\n",
      "luxurious\n",
      "loaded\n",
      "tastefully\n",
      "fireplace\n",
      "beat\n",
      "cottage\n",
      "sedgwick\n",
      "next\n",
      "heart\n",
      "energetic\n",
      "central\n",
      "car\n",
      "three\n",
      "elemantry\n",
      "dryer\n",
      "window\n",
      "kitc\n",
      "priced\n",
      "meaning\n",
      "colored\n",
      "atop\n",
      "garage\n",
      "speakers\n",
      "convenient\n",
      "weekend\n",
      "traditional\n",
      "friendly\n",
      "laminate\n",
      "stanford\n",
      "1922\n",
      "surround\n",
      "self\n",
      "owner\n",
      "lawrence\n",
      "palo\n",
      "formal\n",
      "please\n",
      "bungalow\n",
      "appliances\n",
      "contained\n",
      "ynez\n",
      "allows\n",
      "area\n",
      "grea\n",
      "could\n",
      "recently\n",
      "alamo\n",
      "highway\n",
      "patio\n",
      "depot\n",
      "hall\n",
      "ness\n",
      "boulevard\n",
      "furniture\n",
      "granite\n",
      "ded\n",
      "corner\n",
      "condo\n",
      "apple\n",
      "sunset\n",
      "including\n",
      "there\n",
      "95051\n",
      "cabinetry\n",
      "liberty\n",
      "views\n",
      "expressway\n",
      "glass\n",
      "1118\n",
      "circle\n",
      "dynamic\n",
      "1910\n",
      "quick\n",
      "wonderful\n",
      "design\n",
      "maple\n",
      "privacy\n",
      "washer\n",
      "cole\n",
      "95117\n",
      "out\n",
      "private\n",
      "diamond\n",
      "morning\n",
      "showing\n",
      "avenue\n",
      "north\n",
      "kids\n",
      "lovely\n",
      "desirable\n",
      "marble\n",
      "split\n",
      "though\n",
      "101\n",
      "spaceship\n",
      "having\n",
      "adjacent\n",
      "parking\n",
      "but\n",
      "sleek\n",
      "sliding\n",
      "showpiece\n",
      "front\n",
      "viewed\n",
      "plasma\n",
      "character\n",
      "sound\n",
      "under\n",
      "train\n",
      "95014\n",
      "fabulous\n",
      "perfect\n",
      "breezeway\n",
      "middle\n",
      "baths\n",
      "both\n",
      "was\n",
      "years\n",
      "literally\n",
      "tops\n",
      "community\n",
      "amount\n",
      "beautifully\n",
      "bathrooms\n",
      "very\n",
      "rent\n",
      "not\n",
      "photos\n",
      "balances\n",
      "west\n",
      "newly\n",
      "district\n",
      "square\n",
      "been\n",
      "lane\n",
      "bike\n",
      "kiely\n",
      "frosted\n",
      "saturday\n",
      "plenty\n",
      "sun\n",
      "refrigerator\n",
      "about\n",
      "fallenleaf\n",
      "apartme\n",
      "2865\n",
      "yet\n",
      "hayes\n",
      "its\n",
      "enjoy\n",
      "natural\n",
      "vibrant\n",
      "extended\n",
      "santaclara\n",
      "even\n",
      "alto\n",
      "1390\n",
      "rental\n",
      "junior\n",
      "moved\n",
      "homestead\n",
      "mile\n",
      "280\n",
      "only\n",
      "five\n",
      "off\n",
      "octavia\n",
      "russion\n",
      "more\n",
      "month\n",
      "stainless\n",
      "upper\n",
      "anything\n",
      "minimalist\n",
      "oakland\n",
      "garden\n",
      "castro\n",
      "tri\n",
      "min\n",
      "burning\n",
      "all\n",
      "doors\n",
      "see\n",
      "brand\n",
      "longer\n",
      "separate\n",
      "experience\n",
      "1bath\n",
      "reasonably\n",
      "custom\n",
      "deeded\n",
      "balboa\n",
      "stocked\n",
      "authentic\n",
      "coffee\n",
      "darling\n",
      "diverse\n",
      "flooring\n",
      "ideal\n",
      "maintained\n",
      "7373\n",
      "layout\n",
      "townhomes\n",
      "van\n",
      "147\n",
      "saharasandsapts\n",
      "pets\n",
      "muni\n",
      "storage\n",
      "sized\n",
      "tons\n",
      "village\n",
      "paint\n",
      "tenant\n",
      "high\n",
      "pragmatism\n",
      "exceptional\n",
      "college\n",
      "eichler\n",
      "257\n",
      "wooden\n",
      "NY*NY*NY*NY*NY*NY*NY*NY*NY*NY*NY*NY*NY*NY*NY*NY*NY*NY*NY\n",
      "level\n",
      "location\n",
      "house\n",
      "that\n",
      "walking\n",
      "full\n",
      "school\n",
      "schedule\n",
      "today\n",
      "visit\n",
      "well\n",
      "just\n",
      "bed\n",
      "available\n",
      "studio\n",
      "traditional\n",
      "friendly\n",
      "area\n",
      "could\n",
      "there\n",
      "out\n",
      "but\n",
      "both\n",
      "years\n",
      "rent\n",
      "not\n",
      "been\n",
      "about\n",
      "its\n",
      "even\n",
      "only\n",
      "more\n",
      "experience\n",
      "college\n",
      "proof\n",
      "provide\n",
      "ensuring\n",
      "hvac\n",
      "worker\n",
      "earnings\n",
      "they\n",
      "disabled\n",
      "receive\n",
      "100\n",
      "upon\n",
      "roman\n",
      "facilities\n",
      "skills\n",
      "subject\n",
      "resourceful\n",
      "opportunity\n",
      "human\n",
      "operations\n",
      "doorman\n",
      "ourselves\n",
      "organization\n",
      "dog\n",
      "responsibility\n",
      "little\n",
      "brooklyn\n",
      "clerical\n",
      "part\n",
      "name\n",
      "expert\n",
      "mother\n",
      "work\n",
      "sciences\n",
      "construction\n",
      "tasks\n",
      "cooking\n",
      "interpersonal\n",
      "research\n",
      "driving\n",
      "start\n",
      "bronx\n",
      "automatically\n",
      "domestic\n",
      "helping\n",
      "director\n",
      "creative\n",
      "far\n",
      "retired\n",
      "cash\n",
      "take\n",
      "through\n",
      "learn\n",
      "want\n",
      "win\n",
      "are\n",
      "protestant\n",
      "sharing\n",
      "canvassing\n",
      "income\n",
      "service\n",
      "thumbtack\n",
      "run\n",
      "water\n",
      "hello\n",
      "supplement\n",
      "english\n",
      "assistant\n",
      "simple\n",
      "daniel\n",
      "examples\n",
      "native\n",
      "worked\n",
      "pride\n",
      "like\n",
      "flexible\n",
      "referring\n",
      "history\n",
      "cook\n",
      "conscious\n",
      "sessions\n",
      "placement\n",
      "bio\n",
      "immediately\n",
      "live\n",
      "personal\n",
      "atjmj\n",
      "requiring\n",
      "look\n",
      "mates\n",
      "providing\n",
      "ebay\n",
      "lead\n",
      "flyer\n",
      "preferring\n",
      "highly\n",
      "knowing\n",
      "plumbing\n",
      "chelsea\n",
      "transactions\n",
      "serious\n",
      "500\n",
      "several\n",
      "choir\n",
      "estate\n",
      "metro\n",
      "busin\n",
      "barista\n",
      "deadline\n",
      "executive\n",
      "days\n",
      "based\n",
      "consider\n",
      "brothers\n",
      "term\n",
      "focused\n",
      "positive\n",
      "notch\n",
      "currently\n",
      "program\n",
      "board\n",
      "weekly\n",
      "request\n",
      "liturgically\n",
      "widely\n",
      "nothing\n",
      "background\n",
      "office\n",
      "took\n",
      "customer\n",
      "picking\n",
      "firm\n",
      "hand\n",
      "detail\n",
      "male\n",
      "liturgical\n",
      "here\n",
      "ideally\n",
      "pass\n",
      "hourly\n",
      "essays\n",
      "turn\n",
      "over\n",
      "teachers\n",
      "prompt\n",
      "needs\n",
      "admin\n",
      "dedicated\n",
      "which\n",
      "intern\n",
      "courteous\n",
      "sensitivity\n",
      "basis\n",
      "proofreading\n",
      "keyanna\n",
      "jmj\n",
      "banic\n",
      "standard\n",
      "taught\n",
      "computer\n",
      "evening\n",
      "tutoring\n",
      "recommending\n",
      "since\n",
      "grow\n",
      "such\n",
      "resume\n",
      "treatments\n",
      "attitude\n",
      "some\n",
      "everyone\n",
      "done\n",
      "every\n",
      "many\n",
      "compensation\n",
      "referral\n",
      "intended\n",
      "still\n",
      "check\n",
      "finance\n",
      "doesn\n",
      "teacher\n",
      "200\n",
      "deal\n",
      "job\n",
      "post\n",
      "best\n",
      "per\n",
      "communication\n",
      "free\n",
      "volunteer\n",
      "children\n",
      "working\n",
      "promotion\n",
      "should\n",
      "plants\n",
      "someone\n",
      "handyman\n",
      "medicinal\n",
      "will\n",
      "while\n",
      "sherica\n",
      "few\n",
      "etc\n",
      "various\n",
      "before\n",
      "licensed\n",
      "gain\n",
      "send\n",
      "them\n",
      "generosity\n",
      "betterment\n",
      "chores\n",
      "botany\n",
      "find\n",
      "eager\n",
      "scored\n",
      "money\n",
      "grounded\n",
      "tech\n",
      "downside\n",
      "reliable\n",
      "parents\n",
      "our\n",
      "manager\n",
      "medical\n",
      "near\n",
      "languages\n",
      "field\n",
      "medicine\n",
      "clean\n",
      "type\n",
      "loving\n",
      "socio\n",
      "teaching\n",
      "greetings\n",
      "reach\n",
      "handypro\n",
      "dinner\n",
      "diem\n",
      "writing\n",
      "paid\n",
      "bit\n",
      "company\n",
      "oriented\n",
      "practiced\n",
      "aside\n",
      "get\n",
      "professional\n",
      "apartmemts\n",
      "whatever\n",
      "place\n",
      "herbalism\n",
      "extra\n",
      "real\n",
      "getting\n",
      "deep\n",
      "bonus\n",
      "help\n",
      "sales\n",
      "specialist\n",
      "distribution\n",
      "cleaner\n",
      "cleanings\n",
      "time\n",
      "discussed\n",
      "attention\n",
      "anyone\n",
      "rates\n",
      "care\n",
      "management\n",
      "guidelines\n",
      "willing\n",
      "stripes\n",
      "goal\n",
      "herbal\n",
      "offering\n",
      "would\n",
      "employee\n",
      "believe\n",
      "associate\n",
      "myself\n",
      "reading\n",
      "great\n",
      "filled\n",
      "local\n",
      "starting\n",
      "churches\n",
      "position\n",
      "riverdale\n",
      "bookkeeping\n",
      "code\n",
      "towards\n",
      "general\n",
      "what\n",
      "also\n",
      "editing\n",
      "tradit\n",
      "punctual\n",
      "disinfections\n",
      "female\n",
      "services\n",
      "education\n",
      "fee\n",
      "maintenance\n",
      "much\n",
      "farther\n",
      "how\n",
      "john\n",
      "review\n",
      "online\n",
      "1bed\n",
      "caring\n",
      "study\n",
      "person\n",
      "hrs\n",
      "decided\n",
      "use\n",
      "preferbaly\n",
      "jobs\n",
      "busi\n",
      "purposes\n",
      "further\n",
      "upside\n",
      "possible\n",
      "catholic\n",
      "integri\n",
      "where\n",
      "percentage\n",
      "teams\n",
      "mature\n",
      "non\n",
      "cleaning\n",
      "temporary\n",
      "nursing\n",
      "porter\n",
      "smart\n",
      "good\n",
      "everyday\n",
      "sit\n",
      "going\n",
      "organist\n",
      "treatment\n",
      "students\n",
      "seeking\n",
      "short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "getTopWords(ny, sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
